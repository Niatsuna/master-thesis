\chapter{State-of-the-Art Simulator Analysis}
A comprehensive analysis of the current edge computing simulation landscape is the focus of the following sections.
The analysis provides the foundation for developing a systematic evaluation framework for edge computing simulators by establishing
clear selection criteria for identifying representative simulators from the extensive body of research and development in this domain.
Following a systematic selection process, ten representative edge computing simulators are examined, each offering distinct approaches to modeling edge computing environments.
The chapter proceeds with detailed simulator profiles that introduce their core architectures, capabilities, and intended use cases, providing readers with essential background for understanding their respective strengths and limitations.
To support the systematic comparison of the ten representative simulators, we establish an analysis framework that defines the aspects and categories in which the comparison should take place.
Subsequently, a structured comparative analysis examines these simulators in comparison to their competitors, revealing significant patterns and variations within the simulation landscape.
These significant key aspects are highlighted, and missing aspects are covered in a comprehensive gap analysis, establishing the foundation for the requirements catalogue developed in the subsequent chapter.

% ------------------------------------------
\section{Selection Criteria and Methodology}
As the edge computing simulation landscape is vast, comparing every existing simulator is neither efficient nor sustainable and does not guarantee that meaningful characteristics are highlighted.
Hence, we select a number of representative simulators to represent the current state-of-the-art for the edge computing simulation landscape.

For this, we must determine which criteria a simulator must meet to be chosen, and the number of simulators must be sufficient to provide a comprehensive view of the landscape.

The first and most critical criterion is that every simulator in this list must be published and accessible as an open-source project, so that a meaningful analysis is possible.
If the simulator is not accessible, no insight can be gained nor can the analysis be reproduced or reviewed. 
Additionally, if the simulator is open-source, the approach by which certain features are implemented can be further reviewed and later referenced for one's own implementation.

Secondly, highly cited or well-established simulators should be present in the list of representatives, as a high citation and reference count demonstrates that the simulators have significant impact in research and development.
Furthermore, if a simulator is highly referenced, it can be considered a foundation upon which other researchers and developers have built, and can therefore demonstrate fundamental capabilities that are important for every simulator.

Lastly, we should include simulators that fulfill a meaningful niche or represent a recent trend, as they show the current development direction in the field.
They also demonstrate the recent adoption of modern technologies in the simulation landscape and provide valuable insight into current challenges faced in edge computing.

Based on these three criteria, we select ten representatives, which we introduce in more detail in the next section.

As a general foundation for our research and analysis, we conduct a literature review of the original paper for each simulator as well as any related papers.
This ensures that we have a general theoretical knowledge base for each simulator, as well as their related components, such as simulators they build upon or reference.
Additionally, we examine the official implementation and documentation to gain broader insight into each feature and the architecture of each simulator.
By testing each simulator, we also ensure that each one is still deployable and works as intended, even if it may be older than others.

% ------------------------------------------
\section{Representative Simulator Overview}
In the following each of the ten representatives gets quickly introduced, by stating which criteria they fulfill and by a quick rundown of their capabilities, foci and current development status as well as their strengths and weaknesses.
This section is only to quickly introduce the representatives and a further more detailed rundown is present in the subsequent section as each important capability gets compared.
% ----------------------------------
\subsection{EdgeCloudSim}\label{sec:EdgeCloudSim}
EdgeCloudSim [\cite{sim-edgecloudsim}] is a highly cited and widely adapted edge computing simulator, which focuses on Mobile Edge Computing (MEC) via cloudlets and general edge computing simulation.
With it being first published August 2018, it is one of the older simulators in this list. As of this thesis, EdgeCloudSim was last updated October 2020.

EdgeCloudSim is based on CloudSim [\cite{sim-base-cloudsim}] and presents a modular architecture with advanced mobility modeling in Java.
The general design philosophy is to create a CloudSim-based framework designed for Edge Computing scenarios with a focus on both computational and networking resources.
It supports cloud-edge mobile multi-tier topologies and different network types such as WLAN, WAN, and cellular networks.

EdgeCloudSim is specifically designed for MEC scenarios and demonstrates excellent mobility modeling capabilities.
Additionally, the mobility modeling is supported by realistic network modeling for WLAN and WAN with distance-based latency, and due to its modular architecture, EdgeCloudSim allows easy customization.
However, development halted as of October 2020, and due to its focus on MEC scenarios, the remaining scope is relatively limited.
It is therefore less suitable for pure IoT or cloud computing scenarios and shows limited built-in support for modern artificial intelligence and machine learning workloads.
Furthermore, it is only single-threaded, which leads to scalability limitations, and has no security, privacy, or energy modeling.

% ----------------------------------
\subsection{iFogSim2}\label{sec:iFogSim2}
iFogSim2 [\cite{sim-ifogsim2}] is another highly cited and widely adapted edge computing simulator, which builds upon its previous version iFogSim [\cite{sim-base-ifogsim1}].
It is, just like EdgeCloudSim, based on CloudSim [\cite{sim-base-cloudsim}] and focuses also on MEC scenarios but extends its scope to IoT, fog computing and microservices.
iFogSim2 is developed as a CloudSim-based framework designed for comprehensive Edge/Fog computing scenarios with focus on mobility, clustering and microservice orchestration, and was first published in September 2021.
The development seems to be inactive, as the last meaningful update was done August 2021.

Just as its competitor EdgeCloudSim, it shows an advanced mobility support but provides ways to include real datasets for more realistic approaches.
As for topology, iFogSim2 allows for multi-tier cloud-fog-edge-devices and custom topologies with their respective supported network and protocol types.

iFogSim2 presents comprehensive fog/edge computing simulation capabilities and advance mobility support with it being able to integrate real datasets.
It has strong academic backing and is best used in IoT and fog computing research as well as academic prototyping and clustering algorithm development.
But iFogSim2 also has its weaknesses:
The scalability is limited by its single-threaded execution and it shows limited real-time simulation capabilities.
Furthermore, the output is console-only and needs manual implementation for file output.
It has a limited built-in security modeling and energy modeling capabilities and no native containerization support.
Therefore, it performs relatively poorly for real-time system simulation requirements, massive parallel processing scenarios, pure cloud computing simulations or 
scenarios requiring extensive energy modeling.

% ----------------------------------
\subsection{YAFS}\label{sec:YAFS}
Yet Another Fog Simulator (YAFS) [\cite{sim-yafs}] follows EdgeCloudSim and iFogSim2 as respected foundational simulators in the simulation landscape. First published in July 2019 and last updated June 2022,
YAFS is written in Python 2.7 and extends EdgeSimPy [\cite{sim-base-edgeSimpy}] and focuses on IoT, fog and edge computing as well as resource allocation.
YAFS is designed to be lightweight, robust and highly configurable based on complex network theory with minimal class structure.

It exceeds in being extremly lightweight and being highly configurable and extensible. 
YAFS shows full transparency of simulation data and allows dynamic control of all simulations aspects.
This makes it perfect for research requiring custom algorithms and policies as well as complex network topology analysis and scenarios requiring high customization.

Nonetheless, as YAFS has a relatively inactive development status and is based on a deprecated python version it has some modern flaws.
For example, it has no built-in energy modeling nor security or privacy features.
Just like its competitors, it struggles with being single-threaded and therefore limits its scalability.
With this, YAFS is not ideal for production or commercial environments, large-scale parallel simulations or research that focus on energy or security.

% ----------------------------------
\subsection{EmuFog}\label{sec:EmuFog}
EmuFog [\cite{sim-emufog}] is highly citated, widely adopted and well established extensible emulation framework for large-scale fog computing written in Kotlin and based on MaxiNet\footnote{\url{https://maxinet.github.io/}} [\cite{sim-base-maxinet}] and Mininet\footnote{\url{https://mininet.org/}} [\cite{sim-base-mininet}].
First published in October 2017 and updated until September 2020, EmuFog focuses on large-scale fog computing infrastructure emulation while mainly addressing the network emulation.

It provides highly accurate results and supports docker containerization and both synthetic and real-world topologies.
EmuFog has a good scalable architecture and efficient fog node placement algorithms with cost optimizations, making it a sufficient tool for network performance analysis 
in fog environments, application deployment and orchestration testing as well as large-scale fog infrastructure evaluation.

On the other hand, EmuFog has no mobility nor energy modeling and requires high computational resources for large-scale emulation.
Due to that, large projects are expensive and mobile or energy projects or research is not recommended with EmuFog.
Additionally, its overhead is quite high, making it inefficient for quick prototyping and algorithm development.

% ----------------------------------
\subsection{Mockfog 2.0}\label{sec:Mockfog 2.0}
Mockfog 2.0 [\cite{sim-mockfog2}] is a widely adopted and well established tool in the simulation landscape and is a successor of the previous Mockfog (1.0) [\cite{sim-base-mockfog1}].
It is not a simulator like the previous mentioned, but an cloud-based emulator written in Node.js, which focuses on fog and edge computing for real application testing.
The emulator was first published 2021 and the development halted around June 2021.

Mockfog 2.0 presents a real infrastructure emulation in cloud environments to test fog applications under realistic conditions.
It features docker containerization and automated experiment orchestration while maintaining a real cloud-based emulation with runtime network manipulation.

Due to being a emulation, the resulting behavior is realistic and accurate. 
Additionally, it allows for dynamic, realistic and scalable testing scenarios and reduces manual effort.
With this it succeeds in its main goal of real application testing, but also is useful for validation of prototypes, algorithms and performance.
Additionally, it can be used for infrastructure capacity planning and industrial research making it quite adaptable.

Nevertheless, due to having a cloud infrastructure usage it has high cost and a complex setup.
It is not suitable for large-scale theoretical studies and cannot test theoretical algorithms easily.
Additionally, it has limited mobility support making it inefficient for some projects.

% ----------------------------------
\subsection{Fogify}\label{sec:Fogify}
Fogify [\cite{sim-fogify}] is a Python based emulator that focuses on fog/edge computing in a cloud-native container orchestration based network emulation.
It is designed to be realistic, reproducible and to server scalable fog/edge testbeds using containers.
First published in November 2020, Fogify shows a relatively active development with a modern docker and kubernetes based approach for linux networking.

Due to being realistic and reproducible fog/edge testbeds, it shines at testing real applications in such scenarios.
Furthermore, due to its container and kubernetes integration it supports declarative scenario description and therefore being relatively easy to configure and quite modern.

It features no built-in energy modeling which makes it insufficient for energy-focused research unless the energy-consumption is instrumented externally.
Large-scale and time-accelerated scenarios also struggle with Fogify, as its hardware-limited scalability stands in the way.

% ----------------------------------
\subsection{iContinuum}\label{sec:iContinuum}
iContinuum [\cite{sim-icontinuum}] joins Fogify as a python based emulator that uses docker containerization and kubernetes to emulate linux networking.
Here, it is also designed to be realistic and reproducible, but other than the previous, focuses on cloud-to-edge, orchestration and reproducibility of large-scale experimentations. 

iContinuum succeeds in its goals by serving realistic and reproducible cloud-to-edge continuum testbeds while maintaining a multi-domain orchestration and declarative scenario description.
It is therefore suitable for orchestration research across the cloud-edge continuum, testing real applications in distributed scenarios and in network-based resource emulation.

However, just like its competitor, it is hardware-limited in its scalability and has no synthetic time like simulators in this field.
Additionally, it has no built-in energy modeling, making it not as suitable for large-scale, time-accelerated or energy-focused simulation.

% ----------------------------------
\subsection{FogNetSim++}\label{sec:FogNetSim++}
FogNetSim++ [\cite{sim-fognetsim++}] is relatively well known and established tool written in C++ and first published October 2018.
It is based on the OMNeT++\footnote{\url{https://omnetpp.org/}} and INET\footnote{\url{https://inet.omnetpp.org/}} Framework and focuses on network-centric fog environemnts with distributed fog systems.
The development halted as of the same month.

FogNetSim++ succeeds in advance network modeling with packet-level precision and with the built on proven OMNeT++ framework it shows extensive networking capabilties.
Furthermore, it present excellent scalabilities for large-scale network simulations and broad selection of debugging and visualization tools.
Additionally, it also present deterministic and reproducible simulation results.
This supports network performance analysis and protocol development as well as large-scale network simulation with fog nodes and academic research that requires high network simulation accuracy.

However, due to being highly reliant on OMNeT++ and INET, further development and customization of the simulator require expertise and shows a steep learning curve and complex setup.
Additionally, the documentation is relatively limited and due to the focus primarly laying on networking other components are being neglected.
This makes the simulator relatively inefficient for quick prototyping and real-time system simulation as well as energy- and security-focused studies.

% ----------------------------------
\subsection{FogTorch$\Pi$}\label{sec:FogTorchPi}
FogTorch$\Pi$ [\cite{sim-fogtorchpi}] is a well established probabilistic analysis tool in graph structure application deployment.
Written in Java in May 2017, FogTorch$\Pi$ focuses on fog computing, Quality of Service (QoS) aware deployment optimization as well as IoT application deployment.
With its probabilistic QoS-assurance and resource consumption estimation for eligible deployments of fog application with focus on deployment descision support, it serves a unique focus in the simulation landscape.
No visible development can be observed since January 2019.

It succeeds in clear separation between infrastructure and application modeling as well as cost-aware deployment optimization.
Therefore, it is best used for fog application deployment planning, QoS-aware deployment optimization, cost analysis for fog deployments and general academic research on deployment strategies in the fog computing field.

However, it features no runtime simulation capabilties and the deployment analysis is rather static with no dynamic reconfiguration or adaptation.
Furthermore, no ernergy or security modeling is present and its limited scalability makes it inefficient for energy or security based large-scaled projects.

% ----------------------------------
\subsection{EdgeAISim}\label{sec:EdgeAISim}
EdgeAISim [\cite{sim-edgeAIsim}] is a relatively new simulator, first published in October 2023, and focuses on the growing trend of artificial intelligence on the edge.
It features an extension to EdgeSimPy [\cite{sim-base-edgeSimpy}], which is written in Python, and focuses on resource management and energy optimization for artificial intelligence and machine learning workflows on the edge.

The integration of advanced AI models for intelligent resource management as well as the use of advanced artificial intelligence techniques like multi-armed bandit, Deep Q-Networks (DQN) and GNNs, makes it an 
efficient simulator in regards to scenarios of energy optimization studies, intelligent resource management and task scheduling algorithm development on the edge.

As EdgeAISim is a relatively small extension and has a very strict focus, it lacks in multiple other features that its competitors present.
Due to this, scenarios without artificial intelligence requirements don't reap any benefits from this simulator, as the workflow either hindered by additional workload or is simply 
delegated to EdgeSimPy.

\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{l | c | c | X }
    \hline
    \textbf{Tool} & \textbf{Language} & \textbf{Type} & \textbf{Focus} \\
    \hline\hline
    EdgeCloudSim  & Java & Sim & Mobile Edge Computing, General Edge Computing\\\hline
    iFogSim2      & Java & Sim & IoT, Mobile Edge Computing, Microservices\\\hline
    YAFS          & Python & Sim & IoT, resource allocation, complex networks theory\\\hline
    EmuFog        & Kotlin & Emu & Large-scale fog infrastructure\\\hline
    Mockfog 2.0   & Node.js & Emu & Real application testing, Cloud-based emulation\\\hline
    Fogify        & Python & Emu & Container orchestration, Network emulation\\\hline
    iContinuum    & Python & Emu & Orchestration, Large-scale experimentation\\\hline
    FogNetSim++   & C++ & Sim & Realistic network-centric distributed systems\\\hline
    FogTorch$\Pi$ & Java & Sim & IoT application deployment, QoS-aware deployment optimization\\\hline
    EdgeAISim     & Python & Sim & AI/ML Edge Computing, Resource management, Energy optimization\\\hline
  \end{tabularx}
  \caption{Summary of representatives of the edge computing simulation landscape}
  \label{tab:overview-table-representatives}
  %\vspace{-2mm}
  \small\textit{Note:} Sim = Simulator, Emu = Emulator
\end{figure}

% ------------------------------------------
\section{Comparative Analysis}
In the following, we further dive into each aspect of the representatives and find similarities and differences.
To do that, we first introduce our analysis framework, which covers which aspects we observe, what value they bring to the simulation landscape and what the general structure for our comparison is.
After that we start with the comparison, covering each aspect and dive into further insights that come up during the process.
Additionally, we highlight some common patterns and findings as well as mention some gaps due to missing capabilities, limitations and emerging requirements.

% ----------------------------------
\subsection{Analysis Framework}
To systematically compare each representative with different architectures, foci and implementation approaches, we establish a foundation in form of a framework.
Given that our selected representatives range from simulators to emulators with varying specialized purposes, direct feature-to-feature comparison would be insufficient.
Instead, our framework considers both the specific goals of each representative and the influence of it in the edge computing simulation landscape.
A feature of a fog-edge emulator, may not be a great fit for a cloud-edge simulator, but still can have some insights and useful information regarding implementation or general design.

The framework encompasses five core dimensions that collectively address the complete software development life cycle (SDLC).
This holistic approach ensures our analysis captures not only what they can do, but also how well it serves researches and users across diverse edge computing scenarios.

\subsubsection{Architectural Dimensions}
The architectural foundation determines the flexibility, maintainability and ability to accurately represent edge computing environments.
It concerns itself with the design choices during the development of the simulator.
This can include the choice of the coverage scope, simulation paradigm, the general design philosophy, architecture patterns and integrations capabilities.

Which layer the simulator covers, is important to understand its purpose and capabilities. 
The coverage scope not only determines requirements for protocols and other features, but also different use cases and therefore the general focus.
It further can determine required modeling complexity, affects scalability requirements and shapes varied approaches during development and usage.
Furthermore, some capabilities, requirements and approaches can be essential for a layer in general.

The choice of the simulation paradigm, on the other hand, determines whether simulators employ discrete-event simulation for network dynamics, 
continuous-time modeling for resource consumption, agent-based approaches for autonomous behavior or hybrid combinations that leverage multiple paradigms simultaneously.
As all these different paradigms, can have influence over simulation accuracy, time and resource usage, the selection should be not made hastely.

Hence, the general design philosophy is a criticial point, as it can not only determine the simulation paradigm but also can directly influence the focus of the simulator.
As we established, different foci doesn't mean that no insight can be gained for ther simulators. This is also true for the design philosphy.

Additionally, the choice of a modular or monoltihc architecture pattern, is a foundational choice, as modular designs are known to be more maintainable by separating concerns into independent components, and 
monolithc implementations are deemed to be compacter by integrating all functionality within unified frameworks.

\subsubsection{Functional Capabilities}
Functional capabilities encompass the breadth and depth of the edge computing scenarios each simulator can represent.
This does include general functionalities as well as focus-based capabilities, which needs to be weighted.

The general functionalities encompasses for example network modeling, device and infrastructure modeling and application and workload modeling as well as fault tolerance and reliability modeling and security and privacy concerns.
Each of these functionalities can vary in implementation and approach based on multiple factors, just like if simulation or emulation is presented.

Network modeling assesses support for supported topologies, diverse network types, protocol implementations and realistic network conditions including variable delays, bandwidth constraint and congestion-aware behavior.
Additionally, simulated topologies and other feature can have impact in the network modeling as well, making it a core point in the simulator of choice.

Device and infrastructure modeling examines the range of supported device types from resource-constrained sensors to powerful edge servers, including resource modeling, heterogeneity support for diverse device capabilities and realistic scaling bahavior that reflects hardware constraints and performance characteristics.
The degree in which these device needs to be models are in correlation of the level of detail the simulator wants to accomplished and the focus.

Application and workload modeling investigates support for different application architectures, task dependency patterns, data generation and data flow models.
This includes evaluation of service migration capabilities, workload generation methods and QoS requirement modeling for latency, throughput and reliability constraints.

This gets extended by modeling fault tolerance and checking for reliability of the simulated devices and networks.
If the network modeling concerns itself with the aspects of variable delays, fault tolerance concerns itself with the case of failing nodes and devices that would maybe cause these delays.
By modeling fault tolerance, the simulator can check if the scenario is reliable in cases where devices disconnect. 

Furthermore, security and privacy of the simulator as well as the application are important for personal use as well as developing security-aware applications.
These points can be further extended by focus-based capabilities, as every type of focus can have qualifing capabilities for the edge computing landscape.
For example, if the focus lies on mobile devices, mobility modeling is necessary to model different delay and different data generation based on movement and distance to other nodes.
Energy consumption and sustainability can also be capabilities important for the simulator that need to be observed and weighted based on their respective influence in the edge computing simulation landscape.

\subsubsection{Performance and Scalability}
Performance characteristics directly impact practical utility for large-scale edge computing studies.
A simulator that can't be scaled or performs badly in realistic deployment requirements, can be deemed insufficient.
As scalability is a core aspect of the edge, it is a critical aspect that needs to be analysed for each simulation.

Computational efficiency measures simulation execution speed relative to scenario complexity, memory usage patterns and effectiveness of optimization features including caching, checkpointing and incremental simulation capabilities.

Scalability limits determine practical boundaries for scenario complexity, including maximum supportable network nodes, concurrent applications and realistic scenario sizes before performance degrades unacceptably.
This assessment examines parallelization support through multi-threading, distributed processing or GPU acceleration and evaluates how performance scales with increasing scenario complexity.

\subsubsection{Usability and User Experience}
Usability factors significantly influence simulator adoption and research productivity.

An evaluation of the setup and deployment procress asseses installation complexity, dependency management and documentation quality as well as community support avalability.
This includes examination of platform compatibility, deployment options and the overall barrier to entry for new users.

Programming interface analysis compares configuration methods from graphical interfaces to APIs, evaluating the balance between ease of use and flexibility.
The assessment considers API design quality, example scenario availability and the steepness of the learning curve required for productive use across different users expertise levels.
This does not only include the pure usage of said simulator but also the further development.
Furthermore, it also observes the strictness of the simulator, as a simulator which can only be used for one specific scenario may not be suited in general.

Research workflow integration examines support for experimental design including scenario management, parameter sweep capabilities, reproducibility mechanisms through seed control and deterministic execution and statistical analysis features.
Effective simulators should provide data export capabilities, visualization support and integration with external analysis tools to support rigourous experimental methodologies and interpretation of the results in general.

Extensibility determines a simulator's ability to evolve with changing research needs and support novel edge computing concepts.
This ensures that the simulator grows with the changing simulation landscape, adapt modern approaches and can be customized for a variety of scenarios and use cases.
Hence, this aspect is highly dependent on other factors such as the architecture pattern but can also be achieved and influenced by other internal as well as external features.

Plugin architecture is a common way of adding new functionality without modifiying core code.
To assess this architecture, the availability of well-defined extension points, plugin management systems and the ease of integrationg custom components need to observed.

Programming extensibility evaluates API completeness and choice of programming language for custom development, scripting support for automation and the flexibility of extending exisiting models or creating entirely new behavioral representations.
This includes assessment of inheritance mechanisms, interface design and the complexity required for implementing custom device types, protocols or application models.

Data and metric extensibility investigates capabilities for custom metric definitions, support for novel data patterns, integration with real-world datasets and mechanisms for extending output analysis capabilities.
Robust extensibility should enable researchers to adapt simulators for emerging edge computing paradigms without requiring fundamental architectural changes.

\subsubsection{Validation and Evaluation}
Validation capabilities ensure simulation credibility and enable meaningful research conclusion.
The accuracy of the simulators results is important, as inaccurate results defeat the whole purpose of the process as a whole.

Accuracy and validation asssesment examines built-in verification methods, calibration requirements against real-world data and documented accuracy claims with associated error bounds or precision levelvs.

Benchmarking and comparison evaluates availability of standard benchmark scenarios, reference implementations for comparative analysis and built-in metrics that enable fair evaluation across different approaches.
The assessment considers whether simulators provide validated baseline scenarios and support for reproductin published research results for comparison, review and transparency of the results.

Metrics and analysis support examines the comprehensivesness of built-in measurement capabilities, configurability of output generation and support of custom metric definition.
Effective evaluation support should, as mentioned in previous aspects, include statistical analysis features, data export capabilities in standard formats and visualization tools that facilitate both real-time monitoring and post processing analysis of simulation results. 

% ----------------------------------
\newpage
\subsection{Analysis \& Results}
The prior introduced framework shows a fundamental structure for the following comparison, in which all ten representative tools will be systematically analyzed, compared and organized based on the given categories aspect.

\subsubsection{Architectural Dimensions Comparison}
Starting with the architectural dimensions, the representives demonstrate some architectural diversity and some unity in their approach to edge computing simulation.

\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{ l | l | l | l }
    \hline
    \textbf{Tool} & \textbf{Coverage Scope} & \textbf{Sim. Paradigm} & \textbf{Arch. Pattern} \\
    \hline\hline
    EdgeCloudSim  & Cloud-Edge-Mobile     & DES       & Modular\\\hline
    iFogSim2      & Cloud-Fog-Edge-Device & DES       & Modular\\\hline
    YAFS          & Cloud-Fog-Edge        & DES       & Modular\\\hline
    EmuFog        & Fog-Edge              & Emulation & Modular\\\hline
    Mockfog 2.0   & Cloud-Fog-Edge        & Emulation & Modular\\\hline
    Fogify        & Fog-Edge              & Emulation & Modular\\\hline
    iContinuum    & Cloud-Edge            & Emulation & Modular\\\hline
    FogNetSim++   & Fog-Edge              & DES       & Modular\\\hline
    FogTorch$\Pi$ & Cloud-Fog-Edge        & DES       & Modular\\\hline
    EdgeAISim     & Cloud-Fog-Edge        & DES       & Modular\\\hline
  \end{tabularx}
  \caption{Comparison overview for architectural dimensions}
  \label{tab:analysis-architectural-dimensions}
\end{figure}

As we can see in figure \ref{tab:analysis-architectural-dimensions}, all of the selected representatives demonstrate a relatively clear unity in the choice of architectural pattern, as well as 
the choice of simulation paradigm if they are a simulator and not an emulator.

Every simulator and emulator chose the modular architectural pattern, which allows for easier updates, parallel development and better risk management.
Additionally to that, simulators chose the described standard for simulation paradigms Discrete-Event Simulation (DES) as their respective simulation paradigm.
With the combination of these two, identification of bottlenecks, updating components and optimize resource allocation as well as improve performance and workflow can be easily done.

However, the differences occur in the coverage scope.
As all of the simulators and emulators cover the edge, none is a pure edge concerned simulator/emulator but rather use a hybrid approach with at least two layers, if not more.
Especially, the full-stack of cloud, fog and edge is mostly presented, with the combination of fog and edge alone coming second with a relatively good presented foundation.
The only outlier currently present is iContinuum with a scope of cloud and edge, leaving fog out.

As the boarders between each layer blurs in reality, covering multiple layer does not only make the simulator / emulator more realistic but also supports developers and researchers alike, as not multiple different simulator and emulators need to be intertwined to achieve the goal.
However, the choice of how many layers should be covered, should be considered and studied based on the focus of the simulator / emulator.

Based on the comparison, at least fog and edge should be covered to present not only the devices on the edge but also connectivity. 
The ideal would be the full-stack of cloud, fog and edge to have a complete and modern simulator / emulator that covers all potential applications and projects.

\subsubsection{Functional Capabilities Comparison}
Network modeling capabilities demonstrate significant variation based on each representatives architectural approach and target scenario.

\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{ l | X | X | X  }
    \hline
    \textbf{Tool} & \textbf{Network Types} & \textbf{Supported Protocols} & \textbf{Topology Support}\\
    \hline\hline
    EdgeCloudSim  & WLAN, WAN, Cellular                         & HTTP, Custom                            & Hierarchical \\\hline
    iFogSim2      & WiFi, Ethernet, Cellular, Custom            & TCP/IP, HTTP, MQTT, Custom              & Hierarchical, Custom\\\hline
    YAFS          & Generic/Custom                              & Generic/Custom                          & Any \\\hline
    EmuFog        & WiFi, Ethernet                              & TCP/IP, HTTP, Custom                    & Custom, Real-world datasets\\\hline
    Mockfog 2.0   & Any                                         & Any                                     & Hierarchical, Custom \\\hline
    Fogify        & Ethernet, Wifi, Cellular                    & TCP, HTTP, MQTT                         & Hierarchical, Mesh, Custom \\\hline
    iContinuum    & Ethernet, Wifi, Cellular                    & TCP, HTTP, MQTT                         & Hierarchical, Mesh \\\hline
    FogNetSim++   & WiFi, Ethernet, Cellular, WAN, LAN, Custom  & TCP/IP, UDP, HTTP, Custom               & Hierarchical, Mesh, Tree, Custom \\\hline
    FogTorch$\Pi$ & Abstract                                    & Abstract                                & Hierarchical, Custom \\\hline
    EdgeAISim     & Ethernet, WiFi, Cellular                    & TCP/IP, HTTP, MQTT                      & Hierarchical, Custom \\\hline
  \end{tabularx}
  \caption{Comparison overview of network modeling types}
  \label{tab:analysis-network-modeling-types}
\end{figure}

Topologies are often depicted as hierarchical with the most presented addition of custom topologies.
EdgeCloudSim only supports hierarchical topologies and is therefore the most restrictive simulator in that comparison.
Every other simulator and emulator specifically supports either hierarchical or custom topologies, which includes hierarchical.
In general, most of the simulators and emulators include a way to customize and configure topologies, which leads to customizable scenarios and simulations.
YAFS represents this aspect the most, as its focus on complex network theory allows it to freely deploy any topology.
EmuFog, on the other hand, stands out for its integration of custom as well as real-world datasets as topologies. 
It uses the topology generator BRITE [\cite{BRITE}] for custom topologies and real-world datasets from CAIDA's Resource Catalog\footnote{\url{https://www.caida.org/}}, which makes it highly customizable and realistic as an emulator.

As a general rule, all simulators and emulators support standardized protocols for their respective network types.
EdgeCloudSim simplifies network modeling to focus on mobile edge specifics, rather than additional abstraction of network and protocol types.
This is also the case in some regards for EdgeAISim and emulators like EmuFog, Fogify, and iContinuum.
As the latter are emulators, their respective emulated network is limited.
The only exception is Mockfog 2.0, as it is highly configurable and supports any network type and protocols, with the limitation that the deployed network type and protocols must be supported by the underlying cloud infrastructure.
EdgeAISim is limited in its network capabilities by EdgeSimPy, which, as its base, does not provide extensive protocols or types.
iFogSim2, on the other hand, provides more comprehensive network modeling, as the default supported types are broader and custom network types and protocol types can be achieved.

FogTorch$\Pi$ is an outlier in this regard, as it does not have direct network modeling but rather generic network links with configurable QoS profiles.
The same profiles represent the protocols, making FogTorch$\Pi$ not directly comparable to its competitors as a whole.

\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{l | X | X }
    \hline
    \textbf{Tool} & \textbf{Bandwidth Modeling} & \textbf{Delay Modeling} \\
    \hline\hline
    EdgeCloudSim  & Congestion-aware, configurable per link & Distance-based, dynamic\\\hline
    iFogSim2      & Variable, configurable per link/node    & Propagation, queueing, processing delays\\\hline
    YAFS          & Configurable, supports dynamic changes  & Configurable, supports dynamic changes\\\hline
    EmuFog        & Realistic                               & Realistic\\\hline
    Mockfog 2.0   & Realistic                               & Realistic\\\hline
    Fogify        & Realistic                               & Realistic or configurable \\\hline
    iContinuum    & Configurable per link                   & Configurable per link \\\hline
    FogNetSim++   & Configurable, packet-level              & Configurable, packet-level\\\hline
    FogTorch$\Pi$ & Probabilistic                           & Probabilistic\\\hline
    EdgeAISim     & Configurable per link                   & Configurable per link\\\hline
  \end{tabularx}
  \caption{Comparison overview of bandwidth and delay modeling approaches}
  \label{tab:analysis-network-characteristics}
\end{figure}

Characteristics of network modeling show broader unity than the network modeling itself.
Almost all simulators provide configurable bandwidth modeling per link or node.

Emulators generally depict a realistic network through emulation, while Mockfog 2.0 has further means for configuration.
EdgeCloudSim has congestion-aware bandwidth modeling and, based on its focus on MEC, distance-based and dynamic delay modeling.
With this combination, it presents a robust and realistic approach for network modeling.
iFogSim2 allows for different delay models to be used. 
It supports propagation, queueing, and processing delays, therefore presenting a greater selection for different scenarios.
FogTorch$\Pi$, on the other hand, provides probabilistic means for bandwidth and delay modeling.
It is thereby less granular in detail, as it may not capture fine-grained, event-level interactions like packet collisions and protocol-specific behaviors.
This makes it limited in realism for protocol and network studies.
Additionally, it is rather static and validation may be challenging, as results depend on well-calibrated real-world data.
However, probabilistic modeling allows representation of real-world variability if calibrated correctly and may capture uncertainty in network conditions such as fluctuating bandwidth, random delays, and failures.

Device and infrastructure modeling reveals fundamental differences in abstraction levels and resource representations.
Emulation platforms generally model real hardware constraints through container resource limits, providing authentic resource contention but with limited abstraction capabilities.
Traditional simulators, like EdgeCloudSim, iFogSim2, and FogNetSim++, show detailed and heterogeneous device modeling with CPU, memory, storage, and energy consumption parameters.
While specialized tools, like FogTorch$\Pi$, focus on ML-specific resource characteristics, including GPU and inference accelerators.
EdgeAISim is a combination of both aspects, as its base EdgeSimPy adapts traditional simulators and the AI/ML-based focus of EdgeAISim has some regard for GPU and inference accelerators.
Lastly, YAFS emphasizes computational flexibility through abstract resource containers, making it highly configurable and abstract.

Thus, with the general problem of heterogeneity on the edge, edge computing simulators should provide meaningful ways to model different aspects of devices and their infrastructure to simulate authentic behavior on the edge.

Application and workload patterns also present similar level of variation as the device and infrastructure modeling.

\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{ l | X | X | X | X }
    \hline
    \textbf{Tool} & \textbf{Application Architecture} & \textbf{Task Dependencies}\\
    \hline\hline
    EdgeCloudSim  & Mobile application patterns, Back- and Foreground task classification & Parent-child relationships \\\hline
    iFogSim2      & Monolithic, Microservice                                              & DAG, Precedence constraints \\\hline
    YAFS          & Flexible computational graphs                                         & Configurable graph-based \\\hline
    EmuFog        & Real Containerized                                                    & Docker container linking \\\hline
    MockFog 2.0   & Container Orchestration, Real microservices                           & Container orchestration \\\hline
    Fogify        & Kubernetes, Pod-based                                                 & Kubernetes service meshes\\\hline
    iContinuum    & Abstract Services, Real containerized applications                    & Cross-service communication \\\hline
    FogNetSim++   & Fog-specific                                                          & DAG \\\hline
    FogTorch$\Pi$ & ML inference containers                                               & ML pipeline chains \\\hline
    EdgeAISim     & Neural networks, Federated learning patterns                          & AI pipeline dependencies \\\hline
    
  \end{tabularx}
  \caption{Comparison overview for application architecture and task dependency}
  \label{tab:analysis-application-arch-task-dep}
\end{figure}
Emulation platforms handle real application architectures through emulation.
While EmuFog, Mockfog 2.0 and Fogify built upon their respective containerized approaches with Docker and Kubernetes for real containerized applications, iContinuum brides simulation and emulation with service-oriented application modeling that can represent both abstract services and real containerized application across the computing continuum.
Based on the approach, the task dependencies are handled either via the containerize technology.
Thus, emulators provide reproducible and robust ways of application and task dependencies by building upon standardized and proofed robust modern technologies.

Traditional simulators on the other hand vary heavily in their approaches.
iFogSim2 and FotNetSim++ build upon Directed-Acyclic Graph (DAG) for Task dependencies, but have different application architectures.
FogTorch$\Pi$ and EdgeAISim have a similar relationship, as both present application architecture suited for ML-tasks with taks dependencies relying on pipelines in their respective fields.

Traditional simulators succeed in abstract application modeling flexibilty and differ more in this domain as emulators that provide authentic application execution.
Specialized tools, like EdgeAISim and FogTorch$\Pi$ provde domain-optimized application support with respective pipeline chains and dependencies, while other simulators built upon their respective target and focus group and use established procedures like Directed-acyclic graphs (DAG) and Parent-Child relationships.

\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{ l | X | X }
    \hline
    \textbf{Tool} & \textbf{Data Generation} & \textbf{Data Flow Models}\\
    \hline\hline
    EdgeCloudSim  & Synthetic               & Mobile data patterns, Location-based \\\hline
    iFogSim2      & Synthetic, Integration  & Periodic, Bursty, IoT streams \\\hline
    YAFS          & Synthetic, Integration  & Stream processing, Batch jobs \\\hline
    EmuFog        & Integration             & Real data processing \\\hline
    MockFog 2.0   & Integration             & Real-world data integration \\\hline
    Fogify        & Integration             & Persistent volumes, Config maps \\\hline
    iContinuum    & Integration             & Real-time data routing \\\hline
    FogNetSim++   & Synthetic               & Data aggregation, Filtering \\\hline
    FogTorch$\Pi$ & Limited Synthetic       & Model I/O, Batch processing \\\hline
    EdgeAISim     & Sinthetic, Integration  & Training data, Inference patterns \\\hline
  \end{tabularx}
  \caption{Comparsion overview for data generation and flow models}
  \label{tab:analysis-data-gen-n-flow}
\end{figure}

Data generation is provided either synthetically or with real-world dataset integration.
Emulation in general features emulated real-world data, thereby only offering its integration.
Traditional simulators, on the other hand, always provide a way for synthetic data generation.
EdgeCloudSim has specifically designed workload and Poisson distribution for data generation, while FogNetSim++ focuses on synthetic traffic via OMNeT++ and scenario configuration.
Both of these simulators provide no integration of real-world datasets.
iFogSim2, YAFS, and EdgeAISim, on the other hand, provide means to generate data synthetically and to integrate real-world datasets.
Here, iFogSim2 allows XML configuration and trace-based workloads, YAFS allows for customizable distributions, timestamp arrays, and support of real traces, while EdgeAISim supports AI-driven workload generation and integration of real traces via EdgeSimPy.
An outlier here is FogTorch$\Pi$ again, as it focuses more on deployment and is rather an analysis tool; its data generation is very limited for probabilistic deployment analysis and supports only limited runtime data generation.

The data flow is modeled based on the focus of each project. 
While emulators target real data processing and persistency, simulators target stream processing and inconsistencies in the flow with mobile data patterns and bursty or periodic behavior.
Both kinds of flows represent authentic and realistic data behavior, as real-world data is also inconsistent.

Fault Tolerance and reliability support are demonstrated by varying sophistication levels across all evaluated tools.
\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{ l | X | X | X }
    \hline
    \textbf{Tool} & \textbf{Failure Types} & \textbf{Detection Methods} & \textbf{Recovery Strategies}\\
    \hline\hline
    EdgeCloudSim  & Mobile device, Network failure  & Timeout                       & Task migration \\\hline
    iFogSim2      & Device, Network failure         & Heartbeat, Timeout            & Basic restart \\\hline
    YAFS          & Configurable failures           & Custom detection              & Configurable recovery \\\hline
    EmuFog        & Container failure               & Docker health checks          & Container restart \\\hline
    MockFog 2.0   & Container/Service failure       & Orchestration monitoring      & Auto-restart, Isolation \\\hline
    Fogify        & Pod/Service failure             & Kubernetes probes, Monitoring & Kubernetes healing, Migration \\\hline
    iContinuum    & Service failure                 & Cross-tier monitoring         & Service migration \\\hline
    FogNetSim++   & Hierarchical failure            & Fog-aware monitoring          & Redundancy, Failover \\\hline
    FogTorch$\Pi$ & Model deployment failure        & Health checks                 & Model redeployment \\\hline
    EdgeAISim     & Model degradation, Byzantine    & AI-aware detection            & Model recovery, Consensus \\\hline
  \end{tabularx}
  \caption{Comparison overview for fault tolerance and reliability support}
  \label{tab:analysis-fault-tolerance}
\end{figure}

As figure \ref{tab:analysis-fault-tolerance} presents, emulators mainly focus on container failures that gets detected by monitoring the system and health checks and get recovered by simply restarting the resource.
Traditional simulators on the other hand show a more device focused appoach by checking the state of the device and application via timeouts, heartbeats and health checks or by monitoring the general network for anomalies.
Recovery is often achieved with task migration to other devices or simple restarts of the original failing devices or applications.

Security and privacy is an aspect that varies drastically across the simulation landscape. 
Not only in implementation approaches and covered scenarios but also general existence in some tools.
\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{ l | X | X | X }
    \hline
    \textbf{Tool} & \textbf{Security Mechanisms} & \textbf{Privacy Support} & \textbf{Attack Modeling}\\
    \hline\hline
    EdgeCloudSim  & Secure offloading, Transmission encryption & None & None \\\hline
    iFogSim2      & Authentication, Encryption overhead, Trust models & Basic data anonymization & Limited, custom extensions possible \\\hline
    YAFS          & None, custom extension required & None & None, custom extension equired  \\\hline
    EmuFog        & None, relies on Docker/Mininet & None & None \\\hline
    MockFog 2.0   & None & None & None \\\hline
    Fogify        & None & None & None \\\hline
    iContinuum    & None & None & None \\\hline
    FogNetSim++   & None, relies on OMNeT++/INET & None & None \\\hline
    FogTorch$\Pi$ & None & None & None \\\hline
    EdgeAISim     & None, can simulate AI-based anomaly detection & None & None  \\\hline
  \end{tabularx}
  \caption{Comparison overview of default security and privacy measures}
  \label{tab:analysis-security-n-privacy}
\end{figure}

Most tools do not present default security mechanisms, privacy support, or attack modeling, and abstract these concerns mostly to the application.
The emulators in particular provide no implementation for testing security or privacy in containers and rely heavily on the implemented security measures in their respective technologies. 
They depict attack modeling only if it is implemented in the application itself.
The traditional simulators provide, with the exception of EdgeCloudSim and iFogSim2, no further implementation of security or privacy measures or attack modeling.
YAFS and FogNetSim++, due to their highly customizable nature, show extension points for security and privacy concerns but at the current state do not provide any in the default implementation.
EdgeAISim also provides no default implementation of security and privacy measures, but provides an AI-based anomaly detection simulation, as a way to attempt attack detection.

On the other hand, EdgeCloudSim and iFogSim2 provide relatively basic implementations of security mechanisms, that provide more means to simulate cost and overhead of encryptions and no real protocol-level security, which makes their implemented security relatively limited and mostly for research prototyping.

Mobility support is a relatively focus-based capability for tools to have. 
If the focus does not lie on mobile devices, for example in a static deployment, mobility may not be deemed necessary; on the other hand, multiple projects in the field of edge computing and most notably in the field of IoT devices, show usage of mobile devices and therefore their respective simulators provide necessary means to simulate the network and data characteristics of mobility.
\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{ l | X | X | X | X }
    \hline
    \textbf{Tool} & \textbf{Mobility Models} & \textbf{Handover Support} & \textbf{Location Services} & \textbf{Dynamic Topology}\\
    \hline\hline
    EdgeCloudSim  & Advance cellular models & Advanced  & Location-based services   & User mobility \\\hline
    iFogSim2      & Basic mobility models   & Limited   & Basic location-awareness  & Node mobility \\\hline
    YAFS          & Limited                 & None      & None                      & Dynamic placement \\\hline
    EmuFog        & None                    & None      & None                      & None \\\hline
    MockFog 2.0   & None                    & None      & None                      & None \\\hline
    Fogify        & None                    & None      & None                      & None \\\hline
    iContinuum    & None                    & None      & None                      & None \\\hline
    FogNetSim++   & Limited                 & None      & None                      & Limited  \\\hline
    FogTorch$\Pi$ & None                    & None      & None                      & None \\\hline
    EdgeAISim     & Limited                 & None      & None                      & Limited \\\hline
  \end{tabularx}
  \caption{Comparison overview of mobility modeling}
  \label{tab:analysis-mobility}
\end{figure}

EdgeCloudSim and iFogSim2 provide with their focus on mobile and IoT devices, advanced mobility support for simulations on the edge.
Due to their robustness with handovers, location services, and dynamic topologies, they provide essential features for mobility-focused projects.
YAFS, EdgeAISim, and FogNetSim++ provide limited or user-extendable mobility, but are not as advanced by default as their previously mentioned competitors.
EdgeAISim especially provides no further implementation but relies mainly on its base EdgeSimPy in this regard.
However, every other tool, regardless if emulator or simulator, disregards mobility entirely and therefore provides no implementation for such scenarios.

Energy consumption, as well, is a relatively sporadic deployed feature in the edge computing simulation landscape.
\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{ l | X | X | X }
    \hline
    \textbf{Tool} & \textbf{Energy Modeling Scope} & \textbf{Optimization Support} & \textbf{Granularity}\\
    \hline\hline
    EdgeCloudSim  & None                                & None                    & None \\\hline
    iFogSim2      & Basic device-level energy modeling  & Energy-aware placement  & Per device, per task, limited accuracy \\\hline
    YAFS          & None                                & None                    & None \\\hline
    EmuFog        & None                                & None                    & System, per container \\\hline
    MockFog 2.0   & None                                & None                    & System, per container \\\hline
    Fogify        & None                                & None                    & System, per container \\\hline
    iContinuum    & None                                & None                    & System, per container \\\hline
    FogNetSim++   & None                                & None                    & None \\\hline
    FogTorch$\Pi$ & None                                & None                    & None \\\hline
    EdgeAISim     & AI/ML workload energy modeling      & Energy optimization     & Per task, per device \\\hline
  \end{tabularx}
  \caption{Comparison over of energy consumption modeling}
  \label{tab:analysis-energy}
\end{figure}

Figure \ref{tab:analysis-energy} shows that only iFogSim2 and EdgeAISim provide meaningful implementations for simulating energy consumption for energy-focused scenarios.
However, the emulators do not need to simulate energy consumption, as they emulate a real device and therefore can measure real energy usage externally.
This provides energy consumption based on total system consumption but also per container or pod and can be extended to per task if instrumented in the application.

Contrary to the belief that if we cover mobile devices, we always look out for battery life and energy consumption, EdgeCloudSim does not provide a meaningful energy consumption model.
It can only be approximated by resource utilization based on scenario results, but no direct energy model is implemented.
However, it is mentioned in the documentation of EdgeCloudSim\footnote{\url{https://github.com/CagataySonmez/EdgeCloudSim}} as a needed feature for the simulator.

% ----------------------------------
\subsubsection{Performance and Scalability Comparison}
Performance and scalability are playing a crucial part in software engineering and especially in edge computing simulation, because of the sheer amount of heterogeneous devices which each generates or compute data on the edge.
While comprehensive performance benchmarking across edge computing simulators remain limited, each tools respective paper made some effort to show general performance metrics and some comparisons where executed by other papers. [\cite{s23073492}, \cite{MECHALIKH2025103042}, \cite{fi11030055}]

Computational efficiency is a varying factor across the simulator landscape based on architectural design choices, modeling complexity and implementation optimization strategies.
\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{ l | X | X | X | X }
    \hline
    \textbf{Tool} & \textbf{Simulation Speed} & \textbf{Memory Usage} & \textbf{Optimization Features} & \textbf{Execution Model}\\
    \hline\hline
    EdgeCloudSim  & Moderate  & High      & Caching, Event optimization         & Single-threaded \\\hline
    iFogSim2      & Moderate  & Moderate  & Mobile-specific optimization        & Single-threaded \\\hline
    YAFS          & Fast      & Low       & Parallel processing, Caching        & Multi-threaded \\\hline
    EmuFog        & Slow      & High      & Container optimization              & Real execution \\\hline
    MockFog 2.0   & Slow      & High      & Limited optimization                & Real execution \\\hline
    Fogify        & Moderate  & High      & Kubernetes optimization, Scheduling & Real execution, Kubernetes-based \\\hline
    iContinuum    & Variable  & Moderate  & Adaptive optimization               & Hybrid, Kubernetes-based \\\hline
    FogNetSim++   & Moderate  & Moderate  & Basic optimization                  & Single-threaded \\\hline
    FogTorch$\Pi$ & Fast      & Moderate  & Container optimization              & Container-based \\\hline
    EdgeAISim     & Variable  & High      & GPU acceleration, Model caching     & Hybrid CPU/GPU \\\hline
  \end{tabularx}
  \caption{Comparison overview of computational efficency}
  \label{tab:analysis-performance}
\end{figure}
YAFS demonstrate superior computational efficiency through its custom-built descrete event engine with optimized data structures and parallel processing capabilities.
Traditional simulators that rely on either CloudSim-based frameworks (EdgeCloudSim and iFogSim2), rely heavily on their deploy technologies like FogTorch$\Pi$ and FogNetSim++ or the emulators, are dependent on the performance of their foundations.
The CloudSim-based frameworks show moderate performance limited by the framework's single-threaded architecture and extensive object creation in Java,
while FogTorch$\Pi$ is relies heavily on its containers optimizations and the emulators face inherent efficiency constraints due to real application execution overhead, with Mockfog 2.0 showing the highest resource consumption due to full system emulation.
Furthermore, EdgeAISim suffers additionally from the size of AI models, which results in high memory usage but its performance can vary based on deployed model and scenario.

Additionally to note is, that iFogSim2 acitvely claims to have a 28\% better memory usage than its competitors [\cite{sim-ifogsim2}].

Scalability limits, on the other hand, reveal a critical aspect in the edge computing simulation landscape, as its highly impacted by scenarios, simulation properties and hardware constraints.
The maximum number of supported nodes is often not clearly stated [\cite{qi2025surveyopensourceedgecomputing}], but sometimes implied or in a very few comparison depicted for selected scenarios.

Emulators are typically stated to have a low amount of maximum nodes, as they need to emulate a whole real system rather than simulation only certain aspects.
However, emulators may achieve greater number of nodes by distribution [\cite{qi2025surveyopensourceedgecomputing}, \cite{sim-mockfog2}].
This is especially stated for Mockfog 2.0, as ``MockFogs scalability will usually be limited by the number of VMs that can be provisioned from the cloud provider of choice'' [\cite{sim-mockfog2}].
Otherwise, they mainly rely with their scalability on their underlying technologies and programming languages. This is the case for iContinuum [\cite{sim-icontinuum}].
Fogify and Emufog have both scalability claims, but don't further classify or depict a benchmark for reviewing.
\cite{sim-fogify} states for Fogify that ``the emulation execution environment should be scalable from topologies with a limited number of nodes, capable to run on a single laptop or PC, to hundreds or thousands nodes, running on a whole cluster.'' but don't provide any further necessary
means or valueable benchmark to confirm or deny this claim, while \cite{sim-emufog} states something similar with EmuFog being capable to emulate large network topologies, but doesn't further clarify what this assessment mean nor provide any means to review this claim.

For traditional simulators, some paper show comparison based on a shared scenario and simulation settings or just depict general metrics for their respective simulaotr.
The general consesus is, that iFogSim2 and FogNetSim++ can present up to thousands of nodes on a large-scale basis, with the correct simulation setting.
However, FogNetSim++ will quickly perform worse if the simulation is not correctly fine-tuned, as they simulate a greater network and focus more computational power for this network simulation.
EdgeCloudSim is relatively near this large-scale assessment but falls off because of the same reasons, that the focus on mobility has a higher priority than scalability, making it fall off.
Hence, in more complex scenarios EdgeCloudSim shows a relatively bad scalability but in the right circumstances large-scale scenarios it presents up to thousands of nodes.
FogTorch$\Pi$ and EdgeAISim are mainly limited by their respective workload for AI/ML scenarios and cant be clearly classified.

The most powerful simulator is in this list YAFS, as shown by \cite{sim-yafs} in comparison to iFogSim, with large-scale possible scenarios with thousands of nodes.
Every source, however, puts scalability in high regards and challenge in edge computing simulation.

\subsubsection{Usability and User Experience Comparsion}

The first step in using a tool is its setup and deployment process.
CloudSim-based simulators like iFogSim2 and EdgeCloudSim offer straightforward setup and deployment through standardized Java installation and inclusion of all necessary dependencies.
Their setup documentation is relatively useful and they ship with multiple example scenarios that can be easily deployed to check if the setup was successful.
However, it is notable that iFogSim2 recommends IDEs for Windows usage and EdgeCloudSim's documentation is based on some videos rather than written text.
FogNetSim++ is relatively complex, especially on Windows operating systems, with its key dependencies based on OMNeT++, INET, and C++, and its documentation is relatively limited in comparison to the CloudSim-based simulators.
FogTorch$\Pi$ and YAFS are relatively easy to set up, as they ship with required Python package lists, but YAFS is somewhat outdated due to listed package versions and reliance on a deprecated Python version.
Container-based platforms face significant deployment barriers.
EmuFog requires Linux-specific networking configurations, Mockfog 2.0 demands high-end hardware resources as it emulates a full network, while Fogify and iContinuum require full Kubernetes cluster setup with associated operational complexity.
EdgeAISim presents a different challenge during setup: while the installation process is not complex, the documentation is lacking in some areas and is especially outdated regarding PyTorch and its extensions. Notably, PyTorchs package name has changed, making installation with the provided command incompatible with current Python versions.

As resource requirements vary significantly based on scenario complexity, simulation duration and granularity, modeling fidelity, and implementation approaches, simple stated minimum requirements are not provided in the documentation of any tool directly other than underlying dependencies.
The underlying dependencies primarily depend on the chosen technologies and programming language.
Java-based tools often require Java 8 or higher, while mainly shipping every dependency.
YAFS is developed in a deprecated Python 2.7, but can still be deployed with Python 3.12.4.
EdgeAISim requires at least Python 3.7 and three additional packages for GPU workload as denoted in the documentation.
FogNetSim++ requires, as the only C++ tool, the installation of OMNeT++ and INET.
The emulators only require their containerization technologies, like Docker or an available Kubernetes cluster.
Other requirements such as minimum amount of RAM, CPU cores, etc. are not specified in official documentation or presented in comparative studies.

Community support reveals stark differences in long-term sustainability and user accessibility across the simulator ecosystem.
While highly cited simulators (iFogSim2, EdgeCloudSim, YAFS) show relatively moderate-to-high community activity, other tools are reliant on their respective research domain and, based on their age, have not shown quantifying involvement.
Additionally, underlying technologies may hinder community support as, for example, OMNeT++ and INET as well as most emulators have a high learning curve, making external contributions limited.

However, most tools provide a meaningful way to interact with and configure scenarios as well as the respective tool.
Most of them provide a configuration file, like an XML, YAML, API, or can be configured using a script. 
A majority also feature a hybrid approach, where, for example, topologies can be configured using a configuration file, while other features require direct modification of core code. 
YAFS and EdgeAISim, on the other hand, do not serve any configuration files and are purely Python-based.

Due to their modular architecture, all tools in this list are highly extensible with plugins and further modules with development.
Additionally, their bases, like CloudSim, provide extensions of their own that are compatible with the respective simulator.
This makes every tool on this list able to integrate modern approaches and update their technologies if needed, ensuring their relevancy with emerging technologies in the future.

\subsubsection{Validation and Evaluation Support Comparison}
Validation and evaluation are crucial parts of a tool in the simulation landscape, as the results are the target of a simulation or emulation.
The results need to be readable and meaningful, making metrics an important part of every tool.

Traditional simulator provide customizable metrics that can be defined by the respective user.
However, FogTorch$\Pi$ provides, with its focus on deployment, only deployment metrics, like QoS, cost, and feasibility.
Emulation platforms provide only real system metrics as they emulate the system and can mostly not be further customized.

An interesting point here is where the metrics are defined, raised, and collected.
While iFogSim2 and EdgeAISim provide application-level defined and raised metrics, a majority of the traditional simulators define and raise their metrics on simulator-level, meaning rather than defining each metric in the application on the device that should raise them, simulators 
like EdgeCloudSim and FogNetSim++ define their metrics in their code base and request each metric with the same definition from every simulated device.
FogNetSim++ provides additional scenario-level configuration via OMNeT++ and NED/INI files, but EdgeCloudSim requires modification of the logger embedded in itself.
This requires expertise in EdgeCloudSim and also has the problem of raising metrics that may not be available at the simulated device or in different scenarios.
While application-level raised metrics fix this problem, they may result in more modifications if a specific metric needs to be redefined or updated.
YAFS, on the other hand, is by default simulator-level developed but can be configured with Python scenario code to be close to application-level.
Emulation platforms also provide real system metrics at the application level, as metrics are collected from containers using configurable monitoring tools.

The most standardized metrics in edge computing simulation are performance indicators such as latency and throughput, as well as resource utilization metrics like CPU, memory, and bandwidth usage.

To facilitate result evaluation, visualization tools are often recommended or included with the simulator.
For Kubernetes-based tools, visualization is typically performed using Grafana, while other tools utilize MATLAB\footnote{\url{https://www.mathworks.com/products/matlab.html}} or Python libraries such as Matplotlib\footnote{\url{https://matplotlib.org/}}.
FogNetSim++ is an outlier, as it relies heavily on OMNeT++ for visualization.

The results themselves are exported to readable output.
MockFog 2.0 and EmuFog provide real system logs and cloud API responses that depend on the application for their format.
Fogify and iContinuum have a high error feedback with a combination of Prometheus and Grafana to visualize and monitor the running scenario in a browser.
On the other hand, EdgeCloudSim, YAFS, and FogNetSim++ provide event log files while FogTorch$\Pi$ provides CSV files with deployment metrics.
EdgeAISim provides AI performance metrics with visualization as an image file and iFogSim2 has no default file export implementation, but rather depicts the results in the console.

These results must be reproducible to serve as a valid basis for validation.
To achieve this, all tools provide, to some extent, scenario management for saving, loading, and deterministic execution of scenarios.
The majority of tools provide configuration and scenario files to manage this reproducibility, while YAFS and EdgeAISim apply a seed control approach.
Kubernetes-based emulators such as Fogify and iContinuum additionally guarantee deterministic execution through container orchestration. 

Validation is often not directly provided as an executable implementation, but is instead presented in research papers.
iFogSim2, EdgeCloudSim, and FogNetSim++ have published validation studies and some comparative benchmarking.
YAFS provides validation architecture with custom scripts, while EdgeAISim needs to be user-driven and has no built-in validation.
FogTorch$\Pi$ provides probabilistic validation and deployment feasibility.
Emulation platforms validate against real infrastructure, enhancing authenticity and realism.

% ----------------------------------
\subsection{Key Findings \& Gap Analysis}
The comparative analysis covers each aspect of the lifecycle of a edge computing simulation tool and reveals several patterns, leading implementation but also gaps in the simulation landscape.
In this section we revisit some aspects and briefly state their insight as well as cover some gaps that came up during the comparison of the selected tools.

\subsubsection{Key Findings}
The comparative analysis reveals several significant patterns in the current edge computing simulation landscape:

The architectural analysis reveals a strong convergence towards modular architectural design patterns within all evaluated tools.
This supports not only extensibility and configuration but also maintains and affirms modularity as a key software principle.
Traditional simulators predominantly employ Discrete Event Simulation (DES), whereas emulation-based tools leverage container technology.
The coverage scope typically spans multiple layers of the computing continuum, with fog-edge combinations representing the minimum viable coverage.

Regarding core capabilites, a clear distinction emerges between universal and specialized features.
Network modeling, device and infrastructure modeling and application workload handling, with data generation and task scheduling, are deemed universal standardized capabilities present across all evaluated tools.
However, advanced specialized features such as mobility modeling, energy consumption tracking or security and privacy mechanisms appear selectively and implemented only in tools with the respective focus.

Performance characteristics vary significantly across the landscape.
YAFS demonstrates superior computational efficiency through its optimized discrete event engine and parallel processing capabilities.
iFogSim2 achieves notable memory optimization, reporting 28\% improved memory usage compared to competitors.
Container-based emulators excel in realistic behavior reproduction but encounter inherent scalability limitations.

The analysis also reveals the grade of comprehensive solutions and feature coverage, with iFogSIm2 emerging as the most versatile traditional simulator, offering broad feature coverage.
EdgeCloudSim provides the most robust mobility modeling implementation among evaluated tools and Kubernetes-based emulators deliver the most comprehensive real-world deployment testing capabilities.

\subsubsection{Gap Analysis}
Despite the maturity of existing solutions, several significant gaps persist in the current edge computing simulation landscape:

A critical limitation exists in energy modeling capabilities, as despite energy being a fundamental resource in edge computing, particularly for IoT and mobile devices, comprehensive energy modeling remains largely inadequate.
Most tools either completely lack energy modeling or approximate it through external calculation around resource utilization metrics.
This limitation is particularly notable in mobility-focused simulators like EdgeCloudSim, where battery life should be a crucial consideration.
Key limitations include:
\begin{itemize}
    \item Absence of state-based energy modeling (active vs. idle states)
    \item Insufficient battery lifecycle simulation
    \item Limited application-specific energy profiling
    \item Inadequate consideration of energy as a primary resource constraint
    \item Lack of integration with renewable energy sources or energy harvesting
\end{itemize}

Security and privacy considerations represent another significant gap.
Most tools lack comprehensive security modeling capabilities, often abstracting the concern to the application level.
This limitation results in inadequate support for attack modeling and security validation frameworks, leaving critical aspects of edge computing security unexplored in simulation environments.

Scalability remains a persistent challenge across the simulation landscape.
Single-threaded architectures limit several traditional simulators, while real-world emulation faces inherent hardware resource constraints.
These constraints limit the ability to simulate large-scale scenarios, which are increasingly relevant in real-world deployments.

The validation of tools in the edge computing simulation landscape is inconsistent and limited with a lack of comparative performance metrics.
Making real comparative performance validations complicated and relatively subjective to the deployed scenario.

Finally, with new technologies emerging rapidly, some modern paradigms are limited in their support.
Thus, new fields could become relatively hard to support, such like AI/ML frameworks and blockhcain but also green computing and sustainability metrics.

These findings and gaps highlight both the maturity of the current simulation landscape and areas requiring further development.


