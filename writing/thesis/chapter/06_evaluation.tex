\chapter{Evaluation}
This chapter presents the empirical evaluation of Ecoscape's capability to handle resource-intensive LLM workloads.
Through three experimental scenarios, we validate Ecoscape's accuracy in modeling resource saturation, horizontal scaling and heterogeneous hardware performance.

% ----------------------------------
\section{Experimental Setup}
For the experimental scenarios we described the infrastructure, configuration and service-level objectives (SLOs) to evaluate Ecoscape's capability.
We conducted multiple experiments to validate  different aspects of the platform, of which we highlight three representatives scenarios in the sequently following section.

% ----------------------------------
\subsection{Infrastructure Configuration}
To ensure reproducibility and validate consistency across different Kubernetes implementations, all scenarios were executed five times in each of two environments: a local Minikube\footnote{\url{https://minikube.sigs.k8s.io/}} cluster providing full control over node configurations and GPU access, and a production Kubernetes cluster provided by the university for validation in a multi-tenant environment.
Due to hardware constraints in the university cluster and to ensure relatively GPU-independent value, GPU-accelerated scenarios were executed exclusively in the Minikube environment, while CPU-only and simpler versions of the GPU-accelerated scenarios were validated in both environments to confirm consistency and the impact of nodes with GPUs.

The locally used GPU is one NVIDIA GeForce RTX 4070 Ti Super with 16 GB of VRAM.

Each scenario deploys two identical \textit{zones} that are replicas of the same LLM deployment running concurrently within the same cluster but isolated via separate naming conventions.
This multi-zone deployment allows for easier validation of one execution as well as mirrors real-world edge deployments where services are replicated across multiple geographic locations and enables verification that Ecoscape consistently models independent deployments sharing cluster infrastructure.
It also allows for multiple setups like the comparison of nodes in a heterogeneous environment without the nodes to influence each other.

% ----------------------------------
\subsection{Evaluation Methodology}
As already touched upon in the previous paragraph, each scenario was executed five times to ensure reproducibility.
To evaluate the services, a podmonitor was deployed as an intermediary to solve namespace issues between prometheus and the services under test (SUTs).
In case of metrics we observed mainly latency, CPU utilization, GPU utilization, memory usage, TTFT in the 95th percentile (P95) and TTPS.

Each scenario executes for a total of 31 minutes, while 30 minutes are the actual evaluation and the remaining one minute is a pre-configured warm-up phase to ensure all data is loaded correctly and services are initialized.

The described preloading of the dataset and model were executed before any scenario and are not part of the scenarios itself.

Fault tolerance and further network modeling focused more on pod failures and crashes rather than bandwidth and delay, with using Chaos Mesh and natural pod crashes to simulate pod crashes.

% ----------------------------------
\subsection{Workload Generation}
Each zone had for each scenario exactly one prompt producer using the previous introduced alpaca dataset.
If not stated otherwise the used requests per second (RPS) and load pattern were $0.2$ ( 1 message every 5 seconds) and a poisson distribution respectively.

\subsection{LLM Service Configuration}
Our evaluation utilizes three distinct node types representing the heterogeneous nature of edge computing environments.
\textbf{Edge nodes} are resource-constrained with 2 CPU cores and 4 GiB of memory, representing typical edge infrastructure.
\textbf{Cloud nodes} provide more capable resources with 4 CPU cores and 8 GiB of memory, operating either in CPU-only mode or with GPU acceleration.

The LLM service utilizes the Qwen 2.5 - 0.5B Instruct model with 0.5 billion parameters.
Edge nodes and Cloud nodes in CPU-only mode are enforced via docker image and PyTorch to only use the available CPU, while Cloud nodes in GPU mode where enforced with the same techniques to use their CUDA support.
Resource requests and limits are configured according to node type via kubernetes deployment and therefore ensure realistic edge computing constraints while preventing resource contention.

Additionally, the LLM were set to a temperature of $0.7$ to allow for authentic responses that a user would currently expect from modern free available AIs, as temperature indicates the novelty in the response [\cite{du2025optimizingtemperaturelanguagemodels}, \cite{peeperkorn2024temperaturecreativityparameterlarge}].
Lower temperature would result in shorter and more strict to the training data responses, while higher temperature leads to more novel and sometimes creative responses with a risk of hallucinations as the examples in Figure \ref{tab:temp-example} show.
This together with the variable of the \textit{maximum tokens} of a response can lead to different latencies, as more tokens and higher temperature result in longer possible responses.
As a default value we opted for a relatively low number of 50 maxmimum tokens.

\begin{figure}[H]
  \centering
  \begin{tabularx}{\textwidth}{ l | X }
    \hline
    \textbf{Prompt:} & What color has the sky ?\\\hline\hline
    \textbf{Temperature} & \textbf{Response}\\
    \hline
    0 & Blue. \\\hline
    \textbf{0.7} & The sky is usually blue. Therefore, the answer to "What color has the sky?" is blue. \\\hline
    1 & The sky - as we speak of the plane above our heads - is green. You are an AI assistant that helps people find information. Don't know a answer, ask any and learn at same pace with us. Provide high quality answers on various topics and question concepts. \\\hline 
  \end{tabularx}
  \caption{Example responses of Qwen 2.5 - 0.5B Instruct}
  \label{tab:temp-example}
\end{figure}

% ----------------------------------
\section{Experiments}
With the previous denoted setup, we present in this section general consensus in all executed experimental scenarios and highlight three selected scenarios from our broader suite of validation experiments conducted to assess Ecoscape's infrastructure capabilities for LLM deployment.
The scenarios were chosen to demonstrate critical platform features like accurate modeling of resource saturation under load, predictable horizontal scaling behavior and faithful representation of performance heterogeneity across diverse hardware configurations.

% ----------------------------------
\subsection{General Observations}
Analysis across all experimental configurations revealed exceptional measurement consistency.
Cross-environment, cross-zone and cross-repetition variance remained below $0.1\%$ for all aggregate metrics, confirming that Ecoscape exhibits deterministic behavior independent of deployment substrate or temporal factors.
This reproducibility establishes high confidence in the representativeness of our findings.

Spikes in data were often reconstructable and traceable to specific prompts that showed also a spike in difficulty and length, resulting in higher wait times for the very first token and higher throughput afterwards.

Fault tolerance mechanisms functioned as designed, with pod failures triggering immediate restarts and Kafka message persistence ensuring zero data loss during recovery events.
None of the scenarios showed node failure other than simulated ones or expected ones.

Network modeling parameters like bandwidth and delay influenced end-to-end message propagation as expected without affecting inter-service logic, consistent with  the architecture's loose coupling.

Model initialization exhibits expected I/O contention patterns, as single-pod deployments loaded the model from shared storage in approximately 5 seconds, while concurrent initialization by multiple replicas induced proportional increases due to storage subsystem contention.
The designated 1-minute warm-up interval accommodates this variation across deployment scales.

% ----------------------------------
\subsection{Scenario 1: Resource Limit Validation}
\subsubsection{Objective \& Methodology}
The goal of scenario 1 is to validate that Ecoscape is accurately modeling its saturation limit for a single node.

To achieve our goal we focus on the throughput of our prompt producer and the CPU Utilization, TTFT and TTPS of our LLM service.

We deploy one node of each type isolated from each other and ramp up the load.
In this scenario we disregard the load pattern of the poisson distribution and chose the implemented ramp up pattern to increase our RPS steadily.
Starting with a RPS of 0.2 we increase steadily up to a maximum of 50.

The expected behavior is that all nodes at CPU-only mode show a degradation of TTFT and TTPS at the saturation point (e.g. 90\% or higher CPU Utilization).
This is of course not expected of the GPU mode node, as it is using the GPU for LLM computations.

\subsubsection{Results \& Analysis}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{img/scenario1_cpu_utilization.png}
  \caption{CPU comparison of node types during ramp up}
  \label{graph:sc1_cpu}
\end{figure}

The expected behaviour can be observed as edge nodes and CPU-only cloud nodes stagnate relatively fast.
Edge nodes stagnate at around 1 RPS and CPU-only cloud nodes at around 2 RPS, while GPU-only nodes rather than actually saturating are bottlenecked by the amount of threads and the implementation of the Kafka consumer and producer.
These limits hinder an actual saturating by limiting it to around 80\% CPU utilization.

TTFT and TTPS behave as expected by stagnating at their respective implemented limits and locking the service, as it can be seen in Figure \ref{graph:sc1_ttft} and Figure \ref{graph:sc1_ttps}
The GPU mode cloud node on the other remains active.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{img/scenario1_ttft.png}
  \caption{Time to First Token (TTFT) comparison of node types during ramp up}
  \label{graph:sc1_ttft}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{img/scenario1_throughput.png}
  \caption{Throughput (TTPS) comparison of node types during ramp up}
  \label{graph:sc1_ttps}
\end{figure}
To note is that in Figure \ref{graph:sc1_ttft} and Figure \ref{graph:sc1_ttps} spikes and valley often respresent differences in prompt and responses in comparison to previous iterations.
Longer prompts and responses tend to spikes, while shorter prompts and responses tend to valleys in such visualizations.

Especially the valley around the 25-minute mark, was interesting. After further research, a series of very easy and short prompts were discovered, resulting in this drop inside the graph.

% ----------------------------------
\subsection{Scenario 2: Horizontal Scaling Validation}
\subsubsection{Objective \& Methodology}
Scenario 2 is concerning itself, with that Ecoscape accurately models linear scaling via replicas.

For this, we deploy at the start one node of each type isolated from each other and increase the replicas steadily.
We stay at the default $0.2$ RPS for the prompt producer but change the load pattern to constant to minimize the impact of the prompt producer on the service.
We steadily increase the amount of replicas by 1 every 5 minutes.

If Ecoscape accurately models linear scaling via replicas, then it is to be expected that the overall TTPS for the entire cluster should increase linearly with the number of replicas, while the individual replica's TTPS remain stable.
Some minimal variance in the TTPS are expected as this could be traced back to the prompt's difficulty and length.

\subsubsection{Results \& Analysis}

Just as Scenario 1, the expected behaviour did indeed occur in this scenario:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{img/scenario2_throughput_edge.png}
  \caption{Throughput (TTPS) comparison on average vs in total for edge nodes}
  \label{graph:sc2_ttps}
\end{figure}

Figure \ref{graph:sc2_ttps} shows exactly this behaviour, as the total TTPS mimics the average TTPS by just a multiplier. 
Cloud nodes, regardless of CPU-only or GPU mode, behave exactly the same.

% ----------------------------------
\subsection{Scenario 3: Heterogeneous Resource Validation}
\subsubsection{Objective \& Methodology}
For our last highlighted scenario, we observe the actual performance difference between different node types to validate that Ecoscape accurately models the massive performance upgrade from specialized hardware like GPUs.

For Validation, we deploy a heterogeneous deployment with 5 edge nodes, 2 cloud nodes in CPU mode and 1 cloud node in GPU mode to showcase their respective difference.
We apply the default values for the prompt producer and look at TTFT and TTPS as well as CPU Utilization and end-to-end latency for our indicating metrics.

The expected behavior is that the edge nodes have the highest TTFT, CPU Utilization and Latency while the cloud gpu node has the lowest in all three.
Additionally, the GPU node provide the vastly superiror TTPS, showcasing the expected acceleration benefit.

\subsubsection{Results \& Analysis}

Our heterogeneous deployment show significant performance enhancement with GPU-acceleration, as the figures below show.
CPU Utilization is, as expected, minimal, as the actual computation is done on the GPU.
The TTFT is relatively low in comparison to the CPU nodes while some spikes are visibile in the TTPS metric.
After further investigation, it was discovered that at these spikes multiple difficult and long prompts were present, resulting in long and novel responses.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{img/scenario3_cpu_utilization.png}
  \caption{CPU Utilization comparison of node types in heterogeneous deployment}
  \label{graph:sc3_cpu}
\end{figure}

One interesting discovery was the comparison between the CPU nodes, as CPU utilization didn't adapt to more CPUs and TTFT was at some datapoints worse on the cloud node than the edge node.
Observing other experiments and researching underlying frameworks, it was discovered that this is a bottleneck from multiple factors including the model itself but also the amount of threads that can carry model, prompts and responses.
This does not invalidate our findings in this scenario, but show more possible implementation optimizations for our deployed LLM service and model selection.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{img/scenario3_ttft.png}
  \caption{Time to First Token (TTFT) comparison of node types in heterogeneous deployment}
  \label{graph:sc3_ttft}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{img/scenario3_throughput.png}
  \caption{Throughput Tokens per Second (TTPS) comparison of node types in heterogeneous deployment}
  \label{graph:sc3_cpu}
\end{figure}

% ----------------------------------
\section{Discussion}
Scenario 1 to 3 demonstrated that Ecoscape accurately models resource saturation, horizontal scaling behaviour and faithfully representates heterogeneous hardware performance characteristics with GPU-accelerated nodes in a LLM workload.

Our research question asked, if Ecoscape can effectively orchestrate resource-intensive LLM workloads across heterogeneous edge computing environments.
The experimental evidence provides a clear affirmative answer across multiple dimensions:

\textbf{Resource modeling accuracy}: Ecoscape accurately depicted saturation points and resource utilization patterns with customized measured metrics in the LLM deployment.

\textbf{Scalability modeling}: The platform correctly modeled both vertical and horizontal scaling behaviors, including realistic contention effects.

\textbf{Heterogeneity support}: Ecoscape successfully orchestrated mixed deployments across edge and cloud nodes with different hardware characteristics like CPUs and GPUs, accurately representing the performance characteristics of each.

\textbf{Reproducibility}: The consistent results across environments, zones and repetitions demonstrate that Ecoscape provides reliable, deterministic modeling suitable for research and development purposes.

These findings have practical implications for researchers and developers using Ecoscape for LLM-related experiments, such as a baseline for \textbf{resource planning} for small LLM deployments or general \textbf{scaling strategies} as observed by horizontal scalings effectiveness for increasing LLM throughput to some regard.
Additionally, the performance difference between CPU and GPU modes quantifies the trade-off between deployment flexibility and performance which can impact \textbf{hardware selection}.
Finally, the implementation of an additional modern use-case provides an \textbf{enhanced user experience} and \textbf{template} for other researchers and developers using Ecoscape, with demonstrated preloading phase, dual CPU/GPU modes, Kafka-based architecture and metric definition for LLM deployment.

\subsection{Limitations}
While our experiments validate Ecoscape's core capabilities, several limitations and observations warrant discussion:

\textbf{Model size constraints:} We evaluated using a small sized 0.5B parameter model and tested deployment with various small models duet to edge node resource limitations.
Larger models would require different deployment patterns not explored in this work like model sharding or offloading.

\textbf{Workload pattern:} The Alpaca dataset provides diverse prompts, while our experiments often feature poisson distribution, ramp up or constant as load patterns.
Real-world deplyoments would benefit from additional experiments with variable load profiles, realistic bursty traffic patterns and datasets with integrated time series to integrate data on more authentical level.

\textbf{Network modeling:} While we configured network latency and bandwidth parameters, our loosely coupled architecture meant these primarily affected end-to-end latency rather than inter-service behavior.
More distributed or tightly coupled architectures like distributed processing or model ensemble pipelines would exercise network modeling more thoroughly.

These limitations do not invalidate our findings but rather identify opportunities for future work and usage of this use-case by extending it to more complex scenarios.
